---
logo: "images/logo.png"
execute:
  echo: true
format:
  revealjs: 
    theme: default
    slide-number: true
engine: knitr
---

## Introduction to Arrow in R

![](images/logo.png){fig-align="center" width="500" height="500"}

## Welcome!

Today we're going to cover:

- Working with larger-than-memory datasets with Arrow
- How to get the best performance out in your analyses   
- Where to find more information   

## Workshop format

-   Follow-along coding
-   Gaps to ask question

## Dataset to follow along with

```{r}
#| label: get-data
#| eval: false
options(timeout = 1800)
download.file(
  url = "https://r4ds.s3.us-west-2.amazonaws.com/seattle-library-checkouts.csv",
  destfile = "./data/seattle-library-checkouts.csv"
)
```

## What is Apache Arrow?

::: columns
::: {.column width="50%"}
> A multi-language toolbox for accelerated data interchange and in-memory processing
:::

::: {.column width="50%"}
> Arrow is designed to both improve the performance of analytical algorithms and the efficiency of moving data from one system or programming language to another
:::
:::

::: {style="font-size: 70%;"}
<https://arrow.apache.org/overview/>
:::

## Apache Arrow Specification

In-memory columnar format: a standardized, language-agnostic specification for representing structured, table-like data sets in-memory.

<br>

![](images/arrow-rectangle.png){.absolute left="200"}

## A Multi-Language Toolbox

![](images/arrow-libraries-structure.png)

## Accelerated Data Interchange

![](images/data-interchange-with-arrow.png)

## Accelerated In-Memory Processing

Arrow's Columnar Format is Fast

![](images/columnar-fast.png){.absolute top="120" left="200" height="600"}

::: notes
The contiguous columnar layout enables vectorization using the latest SIMD (Single Instruction, Multiple Data) operations included in modern processors.
:::

## arrow üì¶

<br>

![](images/arrow-r-pkg.png){.absolute top="0" left="300" width="700" height="900"}

## arrow üì¶

![](images/arrow-read-write-updated.png)

## Seattle<br>Checkouts<br>Big CSV

![](images/seattle-checkouts.png){.absolute top="0" left="300"}

## How big is the dataset?

```{r}
#| label: setup
#| echo: false
#| output: false
library(arrow)
library(dplyr)
```

```{r}
#| label: check-file-size
fs::file_size("./data/seattle-library-checkouts.csv")
```

## Opening in Arrow

```{r}
#| label: open-dataset
seattle_csv <- open_dataset(
  sources = "./data/seattle-library-checkouts.csv", 
  format = "csv"
)
```

## Extract schema

```{r}
#| label: extract-schema
schema(seattle_csv)
```

## Arrow Data Types

Arrow has a rich data type system, including direct analogs of many R data types

-   `<dbl>` == `<double>`
-   `<chr>` == `<string>` or `<utf8>`
-   `<int>` == `<int32>`

<br>

<https://arrow.apache.org/docs/r/articles/data_types.html>

## Parsing the Metadata

<br>

Arrow scans üëÄ a few thousand rows of the file(s) to impute or "guess" the data types

::: {style="font-size: 80%; margin-top: 200px;"}
üìö arrow vs readr blog post: <https://thisisnic.github.io/2022/11/21/type-inference-in-readr-and-arrow/>
:::

## Parsers Are Not Always Right

```{r}
#| label: seattle-schema-again
schema(seattle_csv)
```

![](images/data-dict.png){.absolute top="300" left="330" width="700"}

::: notes
International Standard Book Number (ISBN) is a 13-digit number that uniquely identifies books and book-like products published internationally.

Data Dictionaries, metadata in data catalogues should provide this info.
:::

## Let's Control the Schema

<br>

Creating a schema manually:

```{r}
#| label: seattle-schema-write
#| eval: false
schema(
  UsageClass = utf8(),
  CheckoutType = utf8(),
  MaterialType = utf8(),
  ...
)
```

<br>

This will take a lot of typing with 12 columns üò¢

## Let's Control the Schema

```{r}
#| label: open-dataset-schema
seattle_csv <- open_dataset(
  sources = "./data/seattle-library-checkouts.csv", 
  col_types = schema(ISBN = string()),
  format = "csv"
)
```

## Previewing the data

```{r}
#| label: glimpse
seattle_csv |> glimpse()
```

## Arrow dplyr backend

![](images/dplyr-backend.png)

## Querying the data

```{r}
#| label: query
seattle_csv |>
  mutate(IsBook = endsWith(MaterialType, "BOOK")) |>
  select(MaterialType, IsBook)
```

## Preview the query

```{r}
#| label: preview-query
#| code-line-numbers: "|2,5"
seattle_csv |>
  head(20) |>
  mutate(IsBook = endsWith(MaterialType, "BOOK")) |>
  select(MaterialType, IsBook) |>
  collect()
```

## Data manipulation

```{r}
#| label: seattle-data-manip
seattle_csv |>
  filter(endsWith(MaterialType, "BOOK")) |>
  group_by(CheckoutYear) |>
  summarise(Checkouts = sum(Checkouts)) |>
  arrange(CheckoutYear) |> 
  collect()
```

## 9GB CSV file + arrow + dplyr

```{r}
#| label: seattle-dplyr
seattle_csv |>
  filter(endsWith(MaterialType, "BOOK")) |>
  group_by(CheckoutYear) |>
  summarise(Checkouts = sum(Checkouts)) |>
  arrange(CheckoutYear) |> 
  collect()
```

## 9GB CSV file + arrow + dplyr

```{r}
#| label: seattle-dplyr-timed
#| code-line-numbers: "6"
seattle_csv |>
  filter(endsWith(MaterialType, "BOOK")) |>
  group_by(CheckoutYear) |>
  summarise(Checkouts = sum(Checkouts)) |>
  arrange(CheckoutYear) |> 
  collect() |>
  system.time()
```

42 million rows -- not bad, but could be faster....

## File Format: Apache Parquet

![](images/apache-parquet.png){.absolute top="100" left="200" width="700"}

::: {style="font-size: 60%; margin-top: 450px;"}
<https://parquet.apache.org/>
:::

## Parquet

-   usually smaller than equivalent CSV file
-   rich type system & stores the data type along with the data
-   "column-oriented" == better performance over CSV's row-by-row
-   "row-chunked" == work on different parts of the file at the same time or skip some chunks all together

::: notes
-   efficient encodings to keep file size down, and supports file compression, less data to move from disk to memory
-   CSV has no info about data types, inferred by each parser
:::

## Parquet Files: "row-chunked"

![](images/parquet-chunking.png)

## Parquet Files: "row-chunked & column-oriented"

![](images/parquet-columnar.png)

## Writing to Parquet

```{r}
#| label: seattle-write-parquet-setup
#| output: false
#| echo: false
seattle_parquet <- "./data/seattle-library-checkouts-parquet"
```

```{r}
#| label: seattle-write-parquet-single
#| eval: false
seattle_parquet <- "./data/seattle-library-checkouts-parquet"

seattle_csv |>
  write_dataset(path = seattle_parquet,
                format = "parquet")
```

## Storage: Parquet vs CSV

```{r}
#| label: seattle-single-parquet-size
file <- list.files(seattle_parquet)
file.size(file.path(seattle_parquet, file)) / 10**9
```

<br>

Parquet about half the size of the CSV file on-disk üíæ

## File Storage:<br>Partitioning

<br>

::: columns
::: {.column width="50%"}
Dividing data into smaller pieces, making it more easily accessible and manageable
:::

::: {.column width="50%"}

```{r}
#| label: show_dir_tree_1
fs::dir_tree("data/seattle-library-checkouts/")
```

:::
:::

::: notes
also called multi-files or sometimes shards
:::

## Poll: Partitioning?

Have you partitioned your data or used partitioned data before today?

-   1Ô∏è‚É£ Yes
-   2Ô∏è‚É£ No
-   3Ô∏è‚É£ Not sure, the data engineers sort that out!

## Art & Science of Partitioning

<br>

-   avoid files \< 20MB and \> 2GB
-   avoid \> 10,000 files (ü§Ø)
-   partition on variables used in `filter()`

::: notes
-   guidelines not rules, results vary
-   experiment
-   arrow suggests avoid files smaller than 20MB and larger than 2GB
-   avoid partitions that produce more than 10,000 files
-   partition by variables that you filter by, allows arrow to only read relevant files
:::

## Rewriting the Data Again

```{r}
#| label: seattle-write-partitioned
#| eval: false
seattle_parquet_part <- "./data/seattle-library-checkouts"

seattle_csv |>
  group_by(CheckoutYear) |>
  write_dataset(path = seattle_parquet_part,
                format = "parquet")
```

## What Did We "Engineer"?

```{r}
#| label: seattle-partitioned-sizes
seattle_parquet_part <- "./data/seattle-library-checkouts"

sizes <- tibble(
  files = list.files(seattle_parquet_part, recursive = TRUE),
  size_GB = file.size(file.path(seattle_parquet_part, files)) / 10**9
)

sizes
```

## 4.5GB partitioned Parquet files + arrow + dplyr

```{r}
#| label: seattle-partitioned-dplyr-timed
seattle_parquet_part <- "./data/seattle-library-checkouts"

open_dataset(seattle_parquet_part,
             format = "parquet") |>
  filter(endsWith(MaterialType, "BOOK")) |>
  group_by(CheckoutYear) |>
  summarise(Checkouts = sum(Checkouts)) |>
  arrange(CheckoutYear) |> 
  collect() |>
  system.time()
```

<br>

42 million rows -- not too shabby!

## Partition Design

::: columns
::: {.column width="50%"}
-   Partitioning on variables commonly used in `filter()` often faster
-   Number of partitions also important (Arrow reads the metadata of each file)
:::

::: {.column width="50%"}
```{r}
#| label: show_dir_tree
fs::dir_tree("data/seattle-library-checkouts/")
```
:::
:::

## Performance Review: Single CSV

How long does it take to calculate the number of books checked out in each month of 2021?

<br>

```{r}
#| label: seattle-single-csv-dplyr-timed
open_dataset(
  sources = "./data/seattle-library-checkouts.csv", 
  format = "csv"
) |> 
  filter(CheckoutYear == 2021, endsWith(MaterialType, "BOOK")) |>
  group_by(CheckoutMonth) |>
  summarize(TotalCheckouts = sum(Checkouts)) |>
  arrange(desc(CheckoutMonth)) |>
  collect() |>
  system.time()
```

## Performance Review: Partitioned Parquet

How long does it take to calculate the number of books checked out in each month of 2021?

<br>

```{r}
#| label: seattle-parquet-partitioned-dplyr-timed
open_dataset("./data/seattle-library-checkouts",
             format = "parquet") |> 
  filter(CheckoutYear == 2021, endsWith(MaterialType, "BOOK")) |>
  group_by(CheckoutMonth) |>
  summarize(TotalCheckouts = sum(Checkouts)) |>
  arrange(desc(CheckoutMonth)) |>
  collect() |> 
  system.time()
```

## Engineering Data Tips for Improved Storage & Performance

<br>

-   consider "column-oriented" file formats like Parquet
-   consider partitioning, experiment to get an appropriate partition design üóÇÔ∏è
-   watch your schemas üëÄ

## Summary

What we covered:

-   

-   

-   

## Getting help and more resources

<!-- Add in some links here -->

## R for Data Science (2e)

::: columns
::: {.column width="50%"}
![](images/r4ds-cover.jpg){.absolute top="100" width="400"}
:::

::: {.column width="50%"}
<br>

[Chapter 23: Arrow](https://r4ds.hadley.nz/arrow.html)

<br>

<https://r4ds.hadley.nz/>
:::
:::

## Scaling Up with R and Arrow

::: columns
::: {.column width="50%"}
![](images/dummybookcover.png)
:::

::: {.column width="50%"}
[Currently being written but preview available!](https://arrowrbook.com)

<br>

<https://arrowrbook.com>
:::
:::
